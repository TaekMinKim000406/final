{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392220aa-8c04-4ca5-8eda-1cad16eaceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e30435e-1533-4561-922b-62407a644b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "#실행 파일과 같은 폴더에 파일들이 존재해야 합니다.\n",
    "for i in range(1, 10):\n",
    "    file_name = f'기술_과학_{i}.json'  \n",
    "    \n",
    "    if os.path.exists(file_name):  # 파일이 존재하는 경우에만 처리\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        for conversation in data[\"dataset\"].get(\"conversations\", []):\n",
    "            for utterance in conversation.get(\"utterances\", []):\n",
    "                text = utterance.get(\"utterance_text\", \"\")\n",
    "                evaluation = utterance.get(\"utterance_evaluation\", [])\n",
    "                if evaluation:  # 평가 대상이 아닌 문장을 제외\n",
    "                    texts.append(text)\n",
    "                    labels.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d1ef9d-12e7-496c-87f9-7879c09e75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화 코드: 라이브러리 사용\n",
    "import konlpy \n",
    "from konlpy.tag import Okt, Hannanum, Kkma \n",
    "\n",
    "okt = Okt()\n",
    "tokenized_texts = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    tokenized_texts.append(okt.morphs(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728bcb4b-1f0f-43b0-a2f3-4cb8ed4e3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec 과정\n",
    "_word_to_id = {}\n",
    "_id_to_word = {}\n",
    "\n",
    "def preprocess(words): #앞에서 토큰화를 해서 적절히 바꿨습니다. \n",
    "    for word in words:\n",
    "        if word not in _word_to_id:\n",
    "            new_id = len(_word_to_id)\n",
    "            _word_to_id[word] = new_id\n",
    "            _id_to_word[new_id] = word\n",
    "    corpus = np.array([_word_to_id[w] for w in words])\n",
    "    return corpus, _word_to_id, _id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f058064b-b351-4e9c-a3e6-cd4e672aa9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_labels = {\n",
    "    'linguistic_acceptability': [],\n",
    "    'consistency': [],\n",
    "    'interestingness': [],\n",
    "    'unbias': [],\n",
    "    'harmlessness': [],\n",
    "    'no_hallucination': [],\n",
    "    'understandability': [],\n",
    "    'sensibleness': [],\n",
    "    'specificity': []\n",
    "}\n",
    "\n",
    "# 속성별 레이블 추출\n",
    "for i in range(len(labels)):\n",
    "    # 현재 대화에 대한 속성값 초기화\n",
    "    attribute_values = {key: 0 for key in attribute_labels.keys()}\n",
    "\n",
    "    for j in range(len(labels[i])):\n",
    "        for key in attribute_labels.keys():\n",
    "            if labels[i][j].get(key) == 'yes':\n",
    "                attribute_values[key] += 1\n",
    "\n",
    "    # 각 속성별로 모든 발화에서 'yes'인 경우만 1, 아니면 0\n",
    "    for key in attribute_labels.keys():\n",
    "        if attribute_values[key] == len(labels[i]):\n",
    "            attribute_labels[key].append(1)\n",
    "        else:\n",
    "            attribute_labels[key].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c2cf7a8-1a82-48cd-baec-beeb661c5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 참고: 강의 코드. \n",
    "#https://github.com/ssuai/deep_learning_from_scratch2/tree/master/ch06\n",
    "\n",
    "#rnnlm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "from rnnlm import Rnnlm, Rnnlmv2, BetterRnnlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a0e6cee-8823-4a48-9461-cd3bff051131",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 50  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 5     # RNN을 펼치는 크기\n",
    "lr = 20.0\n",
    "max_epoch = 2\n",
    "max_grad = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e543cdb0-50f2-4492-90b3-b013f851bb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n",
      "퍼플렉서티 평가 중 ...\n",
      "999 / 10001\n"
     ]
    }
   ],
   "source": [
    "#RnnLM에서 계층 생성 부분에 마지막에 sigmoid 함수를 추가\n",
    "#rnnlm파일 열어서 아마 TimeSigmoidWithLoss 추가일듯. \n",
    "\n",
    "model = Rnnlm(wordvec_size = wordvec_size, hidden_size = hidden_size )\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "results = []\n",
    "\n",
    "for i in range(len(tokenized_texts)): #마지막은 테스트로 사용\n",
    "    corpus, word_to_id, id_to_word = preprocess(tokenized_texts[i])\n",
    "    xs = corpus[:-1]\n",
    "    ts = corpus[1:]\n",
    "    trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,eval_interval=20)\n",
    "    results.append(eval_perplexity(model, corpus) / len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c770b9f4-d317-44da-ad6a-a8b338ef7417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linguistic_acceptability 0.8636363636363636\n",
      "consistency 에서 에러가 발생\n",
      "interestingness 1.0\n",
      "unbias 에서 에러가 발생\n",
      "harmlessness 에서 에러가 발생\n",
      "no_hallucination 0.9545454545454546\n",
      "understandability 1.0\n",
      "sensibleness 0.9090909090909091\n",
      "specificity 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "results = np.array(results).reshape(-1, 1)\n",
    "\n",
    "for key in attribute_labels.keys():\n",
    "    try: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(results, attribute_labels[key], test_size=0.3, random_state=42)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred  = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(key, accuracy)\n",
    "    except:\n",
    "        print(key, '에서 에러가 발생') #값이 모두 yes인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f5a728a-c684-47c5-b719-1316688f3f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "퍼플렉서티 평가 중 ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TimeSigmoidWithLoss.forward() missing 1 required positional argument: 'ts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\2학기\\딥러닝\\기말과제\\common\\util.py:217\u001b[0m, in \u001b[0;36meval_perplexity\u001b[1;34m(model, corpus, batch_size, time_size)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_flg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Rnnlmv2.forward() got an unexpected keyword argument 'train_flg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     ts \u001b[38;5;241m=\u001b[39m corpus[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     10\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(xs, ts, max_epoch, batch_size, time_size, max_grad,eval_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(corpus))\n\u001b[0;32m     14\u001b[0m results \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(results)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m attribute_labels\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32m~\\Desktop\\2학기\\딥러닝\\기말과제\\common\\util.py:219\u001b[0m, in \u001b[0;36meval_perplexity\u001b[1;34m(model, corpus, batch_size, time_size)\u001b[0m\n\u001b[0;32m    217\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xs, ts, train_flg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    222\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (iters, max_iters))\n",
      "File \u001b[1;32m~\\Desktop\\2학기\\딥러닝\\기말과제\\rnnlm.py:90\u001b[0m, in \u001b[0;36mRnnlmv2.forward\u001b[1;34m(self, xs, ts)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs, ts):\n\u001b[1;32m---> 90\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_layer\u001b[38;5;241m.\u001b[39mforward(score, ts)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\Desktop\\2학기\\딥러닝\\기말과제\\rnnlm.py:86\u001b[0m, in \u001b[0;36mRnnlmv2.predict\u001b[1;34m(self, xs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 86\u001b[0m         xs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xs\n",
      "\u001b[1;31mTypeError\u001b[0m: TimeSigmoidWithLoss.forward() missing 1 required positional argument: 'ts'"
     ]
    }
   ],
   "source": [
    "model = Rnnlmv2(wordvec_size = wordvec_size, hidden_size = hidden_size )\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "results = []\n",
    "\n",
    "for i in range(len(tokenized_texts)): #마지막은 테스트로 사용\n",
    "    corpus, word_to_id, id_to_word = preprocess(tokenized_texts[i])\n",
    "    xs = corpus[:-1]\n",
    "    ts = corpus[1:]\n",
    "    trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,eval_interval=20)\n",
    "    results.append(eval_perplexity(model, corpus) / len(corpus))\n",
    "\n",
    "\n",
    "results = np.array(results).reshape(-1, 1)\n",
    "\n",
    "for key in attribute_labels.keys():\n",
    "    try: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(results, attribute_labels[key], test_size=0.3, random_state=42)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred  = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(key, accuracy)\n",
    "    except:\n",
    "        print(key, '에서 에러가 발생') #값이 모두 yes인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe3f8bc5-36d1-482c-aea0-4c18da858d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "퍼플렉서티 평가 중 ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (350,50) and (100,10000) not aligned: 50 (dim 1) != 100 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     ts \u001b[38;5;241m=\u001b[39m corpus[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     10\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(xs, ts, max_epoch, batch_size, time_size, max_grad,eval_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(corpus))\n\u001b[0;32m     14\u001b[0m results \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(results)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m attribute_labels\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32m~\\Desktop\\2학기\\딥러닝\\기말과제\\common\\util.py:217\u001b[0m, in \u001b[0;36meval_perplexity\u001b[1;34m(model, corpus, batch_size, time_size)\u001b[0m\n\u001b[0;32m    214\u001b[0m         ts[i, t] \u001b[38;5;241m=\u001b[39m corpus[(offset \u001b[38;5;241m+\u001b[39m t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m corpus_size]\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_flg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xs, ts)\n",
      "File \u001b[1;32m~\\Desktop\\2학기\\딥러닝\\기말과제\\rnnlm.py:153\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, xs, ts, train_flg)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs, ts, train_flg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    152\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(xs, train_flg)\n\u001b[1;32m--> 153\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_layer\u001b[38;5;241m.\u001b[39mforward(score, ts)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\Desktop\\2학기\\딥러닝\\기말과제\\rnnlm.py:149\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(self, xs, train_flg)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m    148\u001b[0m     xs \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(xs)\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xs\n",
      "File \u001b[1;32m~\\Desktop\\2학기\\딥러닝\\기말과제\\common\\time_layers.py:273\u001b[0m, in \u001b[0;36mTimeAffine.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    270\u001b[0m W, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[0;32m    272\u001b[0m rx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(N\u001b[38;5;241m*\u001b[39mT, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 273\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mreshape(N, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (350,50) and (100,10000) not aligned: 50 (dim 1) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "model = BetterRnnlm(wordvec_size = wordvec_size, hidden_size = hidden_size )\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "results = []\n",
    "\n",
    "for i in range(len(tokenized_texts)): #마지막은 테스트로 사용\n",
    "    corpus, word_to_id, id_to_word = preprocess(tokenized_texts[i])\n",
    "    xs = corpus[:-1]\n",
    "    ts = corpus[1:]\n",
    "    trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,eval_interval=20)\n",
    "    results.append(eval_perplexity(model, corpus) / len(corpus))\n",
    "\n",
    "\n",
    "results = np.array(results).reshape(-1, 1)\n",
    "\n",
    "for key in attribute_labels.keys():\n",
    "    try: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(results, attribute_labels[key], test_size=0.3, random_state=42)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred  = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(key, accuracy)\n",
    "    except:\n",
    "        print(key, '에서 에러가 발생') #값이 모두 yes인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2e90b-818a-4ca9-9f07-cfbdf57d2635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
